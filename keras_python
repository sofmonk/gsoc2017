from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D
from keras.utils import np_utils
from keras.optimizers import SGD
import numpy as np
from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten
from keras.models import Sequential
from keras import metrics


from keras.models import model_from_json

import pandas as pd


# fix random seed for reproducibility
seed = 7
import numpy as np
np.random.seed(seed)
import pickle

with open('puzz', 'rb') as data:
    puzzles = pickle.load(data)
with open('soll', 'rb') as data:
    solutions = pickle.load(data)

# puzzles = pd.read_csv('/home/ubuntu/PycharmProjects/sudoku_project/foo.csv')
# puzzles.to_pickle('puzz')
puzzles_train = puzzles[0:70000]
x=puzzles_train.as_matrix()
x_small=puzzles[0:10000].as_matrix()
x_small = x_small.reshape((10000, 1, 81, 1))
x_small_test=puzzles[70000:80001].as_matrix()
x_small_test = x_small.reshape((10000, 1, 81, 1))

x_x = x.reshape((70000, 1, 81, 1))


puzzles_test = puzzles[700000:1000000]
x_test=puzzles_test.as_matrix()

solutions_train = solutions[0:70000]

y=solutions_train.as_matrix()
y_y = np_utils.to_categorical(y)
y_small=solutions[0:10000].as_matrix()
y_small_test=solutions[70000:80000].as_matrix()
# y_small=np_utils.to_categorical(y_small)

solutions_test = solutions[700000:1000000]
y_test=solutions_test.as_matrix()




# create model

model = Sequential()
model.add(Dense(input_dim=x.shape[1], output_dim=25,init='uniform', activation='softmax'))

model.add(Dense(input_dim=25, output_dim=50,init='uniform',activation='softmax'))

model.add(Dense(input_dim=50, output_dim=y.shape[1],      init='uniform',activation='softmax'))

model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])
# Fit the model

model.fit(x_x, y, nb_epoch=10,  batch_size=500)

# serialize model to JSON
model_json = model.to_json()
with open("model_08.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model.h5")
print("Saved model to disk")

# later...

# load json and create model
json_file = open('model_08.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("model.h5")
print("Loaded model from disk")
##################################################################
####################################################################################################################################
##################################################################
##################################################################
num_classes=81
cnn = Sequential()
cnn.add(Convolution2D(81, 10, 1, input_shape=(1, 81, 1), border_mode='same', activation='relu'))
cnn.add(Dropout(0.2))
cnn.add(Convolution2D(81,10,1, activation='relu', border_mode='same'))
cnn.add(MaxPooling2D(pool_size=(1, 1)))
cnn.add(Flatten())
cnn.add(Dense(512, activation='relu'))
cnn.add(Dropout(0.5))
cnn.add(Dense(num_classes, activation='softmax'))
# Compile cnn
# epochs = 25
# lrate = 0.01
# decay = lrate/epochs
# sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)
# cnn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
cnn.compile(loss="categorical_crossentropy", optimizer="adam",metrics=['accuracy'])
print(cnn.summary())


cnn.fit(x_small, y_small, nb_epoch=20)#,  batch_size=500)



# evaluate the model
scores = cnn.evaluate(x_small, y_small)
print(scores[1]*100)

y_pred  = cnn.predict(x_small_test)

cnn.evaluate(x_small_test,y_small_test)

accuracy = np.mean(np.equal(np.argmax(y_small_test, axis=-1), np.argmax(y_pred, axis=-1)))


from keras.models import load_model

cnn.save('cnn.h5')
cnn = load_model('cnn.h5')


